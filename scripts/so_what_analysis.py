"""
"SO WHAT?" Analysis Tool - Transform Observations into Insights

"With each conclusion you draw, ask yourself 'So what?'"
This tool helps systematically apply the SO WHAT test to research findings.

Purpose: Force progression from observation â†’ insight â†’ practical implication
"""

from typing import List, Dict, Tuple
from dataclasses import dataclass


@dataclass
class Finding:
    """Represents a research finding with SO WHAT analysis."""
    observation: str
    so_what_1: str  # First-level insight
    so_what_2: str  # Practical implication
    evidence: List[str]
    keywords: List[str]


class SoWhatAnalyzer:
    """
    Analyzes research findings using the SO WHAT methodology.
    """
    
    def __init__(self):
        self.findings = []
    
    def add_finding(self, 
                   observation: str,
                   so_what_1: str,
                   so_what_2: str,
                   evidence: List[str],
                   keywords: List[str]) -> Finding:
        """
        Add a finding with complete SO WHAT chain.
        
        Args:
            observation: Raw observation from experiments
            so_what_1: First-level insight (mechanistic understanding)
            so_what_2: Practical implication (actionable guidance)
            evidence: List of supporting evidence
            keywords: Tags for categorization
        
        Returns:
            Finding object
        """
        finding = Finding(
            observation=observation,
            so_what_1=so_what_1,
            so_what_2=so_what_2,
            evidence=evidence,
            keywords=keywords
        )
        self.findings.append(finding)
        return finding
    
    def generate_report(self, output_path: str = 'SO_WHAT_ANALYSIS.md'):
        """Generate complete SO WHAT analysis report."""
        with open(output_path, 'w') as f:
            f.write("# SO WHAT? Analysis - GDSearch Findings\n\n")
            f.write("**Methodology:** Each finding progresses through three levels:\n")
            f.write("1. **Observation:** What we saw in experiments\n")
            f.write("2. **SO WHAT? (Level 1):** Why this happens (mechanistic insight)\n")
            f.write("3. **SO WHAT? (Level 2):** How practitioners should act (practical implication)\n\n")
            f.write("---\n\n")
            
            for i, finding in enumerate(self.findings, 1):
                f.write(f"## Finding {i}: {', '.join(finding.keywords)}\n\n")
                
                f.write("### ðŸ“Š Observation\n\n")
                f.write(f"{finding.observation}\n\n")
                
                f.write("### ðŸ” SO WHAT? (Level 1: Mechanistic Insight)\n\n")
                f.write(f"{finding.so_what_1}\n\n")
                
                f.write("### ðŸ’¡ SO WHAT? (Level 2: Practical Implication)\n\n")
                f.write(f"{finding.so_what_2}\n\n")
                
                f.write("### ðŸ“š Evidence\n\n")
                for evidence in finding.evidence:
                    f.write(f"- {evidence}\n")
                f.write("\n")
                
                f.write("---\n\n")
        
        print(f"âœ… SO WHAT analysis generated: {output_path}")


def create_gdsearch_findings() -> SoWhatAnalyzer:
    """
    Create SO WHAT analysis for key GDSearch findings.
    """
    analyzer = SoWhatAnalyzer()
    
    # Finding 1: Adam's Speed
    analyzer.add_finding(
        observation="Adam converges faster than SGD+Momentum on Rosenbrock function and MNIST in early epochs.",
        so_what_1="Adam's adaptive learning rate mechanism allows it to take larger steps in directions with consistent gradients (valley floors) while remaining cautious in high-curvature directions (valley walls). This per-parameter scaling is particularly effective in navigating narrow valleysâ€”a common structure in real-world loss landscapes.",
        so_what_2="**Practical implication:** Engineers should prioritize Adam in the early stages of training to quickly identify promising regions of the parameter space. This accelerates the prototyping cycle, allowing faster iteration on model architecture and hyperparameters. However, monitor generalization gap after ~5-10 epochs.",
        evidence=[
            "Trajectory plots: `plots/adam_trajectory_grid_rosenbrock.png` show rapid progress",
            "Generalization gap plots: `plots/*_gen_gap.png` show early convergence",
            "Quantitative summary: AdamW achieves 95%+ test accuracy by epoch 3"
        ],
        keywords=["Adam", "Convergence Speed", "Adaptive Methods"]
    )
    
    # Finding 2: Generalization Gap
    analyzer.add_finding(
        observation="SGD+Momentum achieves slightly lower test accuracy initially but has significantly smaller generalization gap (0.08 vs 0.15 for AdamW) by epoch 20.",
        so_what_1="This supports the 'sharp vs flat minima' hypothesis. Adam's fast, adaptive steps allow it to settle quickly into sharp minima (steep local curvature). SGD+Momentum's momentum carries it past sharp minima, preferring flatter basins that generalize better. The generalization gap difference (~0.07) indicates AdamW overfits more to training data.",
        so_what_2="**Practical implication:** For production models where generalization is critical, consider a hybrid strategy: (1) Use Adam for first 30-50% of training to find good regions quickly, (2) Switch to SGD+Momentum for final training to refine into flatter, better-generalizing minima. This combines speed and quality.",
        evidence=[
            "Loss landscape plots: `plots/loss_landscape_*.png` show flatter neighborhoods for SGD+Momentum",
            "Summary table: `results/summary_quantitative.csv` shows gen-gap 0.08 vs 0.15",
            "Test accuracy: Both achieve ~97.5%, but SGD+Momentum with smaller gap"
        ],
        keywords=["Generalization", "Sharp Minima", "Flat Minima", "SGD+Momentum"]
    )
    
    # Finding 3: Momentum Instability
    analyzer.add_finding(
        observation="High momentum (Î²=0.99) with high learning rate (lr=0.01) causes divergence on Rosenbrock function.",
        so_what_1="Momentum acts as a 'memory' with effective length ~1/(1-Î²). At Î²=0.99, the effective memory is 100 steps. When combined with high LR and high local curvature (Rosenbrock valley walls), small gradient errors accumulate and amplify over 100 steps, leading to exponential divergence. This is a geometric interaction between memory length and landscape conditioning.",
        so_what_2="**Practical implication:** When using high momentum (Î² > 0.9), reduce learning rate proportionally. Rule of thumb: lr_max â‰ˆ 0.001 * (1-Î²). This prevents momentum-amplified instabilities in early training. Also explains why learning rate warmup is critical with high momentumâ€”it prevents early divergence before momentum accumulator stabilizes.",
        evidence=[
            "Failed runs: OverflowError at iteration ~50 with Î²=0.99, lr=0.01",
            "Successful runs: Stable convergence with Î²=0.99, lr=0.001",
            "Hessian eigenvalues: Condition number ~1000 in Rosenbrock valley"
        ],
        keywords=["Momentum", "Instability", "Learning Rate", "Ill-Conditioning"]
    )
    
    # Finding 4: Per-Layer Gradients
    analyzer.add_finding(
        observation="AdamW maintains uniform gradient distribution across layers throughout training. SGD+Momentum shows layer imbalance early (emphasizing later layers) that equalizes over time.",
        so_what_1="Later layers (closer to loss) naturally receive stronger error signals due to backpropagation's chain rule. AdamW's per-parameter adaptive scaling (dividing by âˆšv) normalizes this imbalance automatically. SGD+Momentum propagates raw gradients, maintaining the natural imbalance until momentum accumulation smooths it out over many iterations.",
        so_what_2="**Practical implication:** This explains why AdamW works well 'out-of-the-box' with default hyperparameters, while SGD+Momentum often requires careful tuning (layer-wise learning rates, gradient clipping, longer warmup). For new architectures, start with AdamW. For well-understood architectures, invest time tuning SGD+Momentum for better generalization.",
        evidence=[
            "Bar charts: `plots/*_layer_grads.png` at epochs [1, 10, 20]",
            "AdamW: Uniform ~0.1-0.2 across all layers, all epochs",
            "SGD+Momentum: Layer 2 (final) has 5x larger grad_norm than Layer 0 at epoch 1"
        ],
        keywords=["Per-Layer Gradients", "Adaptive Scaling", "Layer Imbalance"]
    )
    
    # Finding 5: Saddle Escape
    analyzer.add_finding(
        observation="On SaddlePoint function, optimizers spend 15-30% of iterations near saddle regions (Î»_min Ã— Î»_max < 0). Adam escapes faster than SGD.",
        so_what_1="Saddle points are characterized by indefinite Hessians (mixed-sign eigenvalues). At saddles, gradients approach zero, but momentum helps SGD maintain progress. Adam's adaptive per-dimension scaling takes larger steps in negative-curvature directions (small second moment), accelerating escape. This validates theoretical analysis of adaptive methods' saddle-escaping properties.",
        so_what_2="**Practical implication:** This supports using small batch sizes or gradient noise during training. Noise helps escape saddles by breaking symmetry. Also justifies adaptive methods (Adam/RMSProp) for high-dimensional problems where saddles are ubiquitousâ€”they naturally overcome saddle sticking without manual intervention.",
        evidence=[
            "Hessian eigenvalue tracking: `lambda_min Ã— lambda_max < 0` for 15-30% of iterations",
            "Saddle escape analysis: `plot_eigenvalues.py` quantifies time in saddle regions",
            "Theoretical papers: Ge et al. (2015) on saddle points in non-convex optimization"
        ],
        keywords=["Saddle Points", "Eigenvalues", "Escape Dynamics"]
    )
    
    # Finding 6: Loss Landscape Curvature
    analyzer.add_finding(
        observation="1D and 2D loss landscape slices around trained weights show flatter neighborhoods for SGD+Momentum compared to AdamW.",
        so_what_1="This directly visualizes the sharp vs flat minima hypothesis. Flat minima have low curvature (small Hessian eigenvalues), meaning loss doesn't change rapidly with small parameter perturbations. Sharp minima have high curvature. Flat minima generalize better because they're more robust to parameter perturbationsâ€”which is effectively what happens with different test data (different loss landscape slices).",
        so_what_2="**Practical implication:** When fine-tuning pretrained models, prefer SGD+Momentum over Adam to avoid destroying the flat structure of the pretrained weights. Also, when deploying models to production (where quantization/pruning may perturb weights), train with SGD+Momentum for more robust minima. For research models where generalization is measured carefully, SGD+Momentum is the gold standard.",
        evidence=[
            "1D slices: `plots/loss_landscape_1d.png` show wider flat regions for SGD+Momentum",
            "2D surfaces: `plots/loss_landscape_2d_surface.png` show smoother bowls",
            "Hessian analysis: Lower average condition number for SGD+Momentum at convergence"
        ],
        keywords=["Loss Landscape", "Flat Minima", "Curvature", "Generalization"]
    )
    
    return analyzer


def main():
    """Generate SO WHAT analysis report."""
    print("=" * 60)
    print("SO WHAT? Analysis Generator")
    print("=" * 60)
    
    analyzer = create_gdsearch_findings()
    analyzer.generate_report()
    
    print(f"\nðŸ“Š Analysis Statistics:")
    print(f"  Total findings: {len(analyzer.findings)}")
    print(f"  Observation â†’ Insight â†’ Implication chains: {len(analyzer.findings)}")
    
    print(f"\nðŸ’¡ This analysis demonstrates:")
    print(f"  âœ… Systematic progression from observation to insight")
    print(f"  âœ… Clear practical implications")
    print(f"  âœ… Evidence-backed conclusions")
    print(f"  âœ… Actionable guidance for practitioners")
    
    print(f"\nðŸ“– Use this in:")
    print(f"  - Discussion section of paper")
    print(f"  - Presentation slides")
    print(f"  - Technical blog posts")
    print(f"  - Practitioner guidelines")


if __name__ == '__main__':
    main()
