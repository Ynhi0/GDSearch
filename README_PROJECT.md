# GDSearch - So sÃ¡nh Thuáº­t toÃ¡n Tá»‘i Æ°u hÃ³a

Dá»± Ã¡n Python chuyÃªn nghiá»‡p Ä‘á»ƒ so sÃ¡nh cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a (SGD, SGD Momentum, RMSProp, Adam) trÃªn cÃ¡c hÃ m kiá»ƒm tra 2D.

## ğŸ“‹ MÃ´ táº£

Dá»± Ã¡n nÃ y triá»ƒn khai vÃ  so sÃ¡nh hiá»‡u suáº¥t cá»§a cÃ¡c thuáº­t toÃ¡n gradient descent khÃ¡c nhau trÃªn cÃ¡c hÃ m kiá»ƒm tra tá»‘i Æ°u hÃ³a cá»• Ä‘iá»ƒn:
- **Rosenbrock**: HÃ m cÃ³ thung lÅ©ng háº¹p, khÃ³ tá»‘i Æ°u hÃ³a
- **Ill-Conditioned Quadratic**: HÃ m báº­c hai vá»›i sá»‘ Ä‘iá»u kiá»‡n cao
- **Saddle Point**: HÃ m cÃ³ Ä‘iá»ƒm yÃªn ngá»±a

## ğŸ—‚ï¸ Cáº¥u trÃºc Dá»± Ã¡n

```
GDSearch/
â”œâ”€â”€ test_functions.py      # Äá»‹nh nghÄ©a cÃ¡c hÃ m kiá»ƒm tra
â”œâ”€â”€ optimizers.py          # Triá»ƒn khai cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u
â”œâ”€â”€ run_experiment.py      # Script cháº¡y thÃ­ nghiá»‡m
â”œâ”€â”€ plot_results.py        # Script trá»±c quan hÃ³a káº¿t quáº£
â”œâ”€â”€ requirements.txt       # CÃ¡c thÆ° viá»‡n phá»¥ thuá»™c
â”œâ”€â”€ results/              # ThÆ° má»¥c chá»©a káº¿t quáº£ CSV
â”œâ”€â”€ plots/                # ThÆ° má»¥c chá»©a biá»ƒu Ä‘á»“
â””â”€â”€ README.md             # File nÃ y
```

## ğŸš€ CÃ i Ä‘áº·t

### 1. Clone hoáº·c táº£i dá»± Ã¡n

```bash
cd /workspaces/GDSearch
```

### 2. CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n phá»¥ thuá»™c

```bash
pip install -r requirements.txt
```

CÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:
- `numpy`: TÃ­nh toÃ¡n sá»‘ há»c
- `matplotlib`: Váº½ Ä‘á»“ thá»‹
- `pandas`: Xá»­ lÃ½ dá»¯ liá»‡u
- `tqdm`: Hiá»ƒn thá»‹ thanh tiáº¿n trÃ¬nh

## ğŸ“Š Sá»­ dá»¥ng

### BÆ°á»›c 1: Cháº¡y ThÃ­ nghiá»‡m

Cháº¡y táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m so sÃ¡nh:

```bash
python run_experiment.py
```

Script nÃ y sáº½:
- Cháº¡y 4 thuáº­t toÃ¡n tá»‘i Æ°u (SGD, SGD Momentum, RMSProp, Adam)
- TrÃªn 3 hÃ m kiá»ƒm tra khÃ¡c nhau
- Vá»›i 2 learning rates khÃ¡c nhau (0.01, 0.001)
- Vá»›i 3 seed khÃ¡c nhau Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh á»•n Ä‘á»‹nh
- Tá»•ng cá»™ng: **72 thÃ­ nghiá»‡m**

Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `results/` dÆ°á»›i dáº¡ng file CSV.

### BÆ°á»›c 2: Trá»±c quan hÃ³a Káº¿t quáº£

Táº¡o cÃ¡c biá»ƒu Ä‘á»“ tá»« káº¿t quáº£ thÃ­ nghiá»‡m:

```bash
python plot_results.py
```

Script nÃ y sáº½ táº¡o:
- **Biá»ƒu Ä‘á»“ quá»¹ Ä‘áº¡o**: Hiá»ƒn thá»‹ Ä‘Æ°á»ng Ä‘i cá»§a thuáº­t toÃ¡n trÃªn khÃ´ng gian 2D
- **Biá»ƒu Ä‘á»“ metrics**: Loss, Gradient Norm, Update Norm theo thá»i gian
- **Biá»ƒu Ä‘á»“ so sÃ¡nh**: So sÃ¡nh trá»±c tiáº¿p giá»¯a cÃ¡c thuáº­t toÃ¡n

Táº¥t cáº£ biá»ƒu Ä‘á»“ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `plots/`.

## ğŸ”¬ Chi tiáº¿t Ká»¹ thuáº­t

### HÃ m Kiá»ƒm tra

#### 1. Rosenbrock
$$f(x,y) = (a - x)^2 + b(y - x^2)^2$$

- Tham sá»‘ máº·c Ä‘á»‹nh: a=1, b=100
- Äiá»ƒm cá»±c tiá»ƒu: (1, 1)
- Äáº·c Ä‘iá»ƒm: Thung lÅ©ng háº¹p, khÃ³ tá»‘i Æ°u

#### 2. Ill-Conditioned Quadratic
$$f(x,y) = 0.5 \times (\kappa x^2 + y^2)$$

- Tham sá»‘ máº·c Ä‘á»‹nh: Îº=100
- Äiá»ƒm cá»±c tiá»ƒu: (0, 0)
- Äáº·c Ä‘iá»ƒm: Äiá»u kiá»‡n xáº¥u, hÃ¬nh elip dÃ i

#### 3. Saddle Point
$$f(x,y) = 0.5 \times (x^2 - y^2)$$

- Äiá»ƒm yÃªn ngá»±a: (0, 0)
- Äáº·c Ä‘iá»ƒm: KhÃ´ng cÃ³ cá»±c tiá»ƒu toÃ n cá»¥c

### Thuáº­t toÃ¡n Tá»‘i Æ°u

#### 1. SGD (Stochastic Gradient Descent)
```
Î¸_new = Î¸_old - lr Ã— gradient
```

#### 2. SGD Momentum
```
v_new = Î² Ã— v_old + gradient
Î¸_new = Î¸_old - lr Ã— v_new
```

#### 3. RMSProp
```
s_new = Ï Ã— s_old + (1-Ï) Ã— gradientÂ²
Î¸_new = Î¸_old - lr Ã— gradient / âˆš(s_new + Îµ)
```

#### 4. Adam
```
m_new = Î²â‚ Ã— m_old + (1-Î²â‚) Ã— gradient
v_new = Î²â‚‚ Ã— v_old + (1-Î²â‚‚) Ã— gradientÂ²
m_hat = m_new / (1 - Î²â‚^t)
v_hat = v_new / (1 - Î²â‚‚^t)
Î¸_new = Î¸_old - lr Ã— m_hat / (âˆšv_hat + Îµ)
```

## ğŸ“ˆ PhÃ¢n tÃ­ch Káº¿t quáº£

Má»—i thÃ­ nghiá»‡m lÆ°u trá»¯:
- **iteration**: Sá»‘ vÃ²ng láº·p
- **x, y**: Tá»a Ä‘á»™ tham sá»‘ táº¡i má»—i bÆ°á»›c
- **loss**: GiÃ¡ trá»‹ hÃ m má»¥c tiÃªu
- **grad_norm**: Chuáº©n cá»§a gradient
- **update_norm**: Chuáº©n cá»§a bÆ°á»›c cáº­p nháº­t
- **grad_x, grad_y**: CÃ¡c thÃ nh pháº§n gradient

## ğŸ¯ TÃ¹y chá»‰nh

### Thay Ä‘á»•i cáº¥u hÃ¬nh thÃ­ nghiá»‡m

Chá»‰nh sá»­a hÃ m `create_experiment_configs()` trong `run_experiment.py`:

```python
# ThÃªm learning rate má»›i
optimizers = [
    {'type': 'Adam', 'params': {'lr': 0.0001}},  # Learning rate nhá» hÆ¡n
    # ...
]

# Thay Ä‘á»•i sá»‘ vÃ²ng láº·p
num_iterations = 2000  # TÄƒng sá»‘ vÃ²ng láº·p
```

### ThÃªm hÃ m kiá»ƒm tra má»›i

Táº¡o lá»›p má»›i trong `test_functions.py`:

```python
class MyFunction(TestFunction):
    def compute(self, x, y):
        # Triá»ƒn khai hÃ m cá»§a báº¡n
        return ...
    
    def gradient(self, x, y):
        # Triá»ƒn khai gradient
        return grad_x, grad_y
    
    def hessian(self, x, y):
        # Triá»ƒn khai Hessian
        return np.array([[h_xx, h_xy], [h_xy, h_yy]])
```

### ThÃªm thuáº­t toÃ¡n tá»‘i Æ°u má»›i

Táº¡o lá»›p má»›i trong `optimizers.py`:

```python
class MyOptimizer(Optimizer):
    def __init__(self, lr=0.01):
        self.lr = lr
        # Khá»Ÿi táº¡o tráº¡ng thÃ¡i
    
    def step(self, params, gradients):
        # Triá»ƒn khai logic cáº­p nháº­t
        return new_x, new_y
    
    def reset(self):
        # Reset tráº¡ng thÃ¡i
        pass
```

## ğŸ“ VÃ­ dá»¥ Sá»­ dá»¥ng Module

### Sá»­ dá»¥ng trá»±c tiáº¿p trong code

```python
from test_functions import Rosenbrock
from optimizers import Adam

# Khá»Ÿi táº¡o
func = Rosenbrock(a=1, b=100)
opt = Adam(lr=0.001)

# Äiá»ƒm báº¯t Ä‘áº§u
x, y = -1.0, 2.0

# Tá»‘i Æ°u hÃ³a
for i in range(1000):
    loss = func.compute(x, y)
    grad_x, grad_y = func.gradient(x, y)
    x, y = opt.step((x, y), (grad_x, grad_y))
    
    if i % 100 == 0:
        print(f"Iteration {i}: loss = {loss:.6f}")
```

## ğŸ› Troubleshooting

### Lá»—i: Module not found
```bash
# Äáº£m báº£o báº¡n Ä‘ang á»Ÿ Ä‘Ãºng thÆ° má»¥c
cd /workspaces/GDSearch

# CÃ i Ä‘áº·t láº¡i dependencies
pip install -r requirements.txt
```

### ThÆ° má»¥c results trá»‘ng
```bash
# Cháº¡y thÃ­ nghiá»‡m trÆ°á»›c
python run_experiment.py
```

### KhÃ´ng hiá»ƒn thá»‹ biá»ƒu Ä‘á»“
```bash
# Kiá»ƒm tra matplotlib backend
python -c "import matplotlib; print(matplotlib.get_backend())"
```

## ğŸ“š TÃ i liá»‡u Tham kháº£o

- **SGD**: Robbins & Monro (1951)
- **Momentum**: Polyak (1964)
- **RMSProp**: Tieleman & Hinton (2012)
- **Adam**: Kingma & Ba (2014)

## ğŸ¤ ÄÃ³ng gÃ³p

Äá»ƒ thÃªm tÃ­nh nÄƒng má»›i hoáº·c bÃ¡o lá»—i, vui lÃ²ng:
1. Fork dá»± Ã¡n
2. Táº¡o branch má»›i
3. Commit thay Ä‘á»•i
4. Táº¡o Pull Request

## ğŸ“„ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t hÃ nh dÆ°á»›i giáº¥y phÃ©p MIT.

## ğŸ‘¤ TÃ¡c giáº£

Dá»± Ã¡n Ä‘Æ°á»£c táº¡o ra Ä‘á»ƒ nghiÃªn cá»©u vÃ  so sÃ¡nh cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a trong machine learning.

---

**LÆ°u Ã½**: Dá»± Ã¡n nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  nghiÃªn cá»©u. Äá»ƒ sá»­ dá»¥ng trong production, cÃ¢n nháº¯c thÃªm cÃ¡c tÃ­nh nÄƒng nhÆ° validation, error handling, vÃ  logging chi tiáº¿t hÆ¡n.
